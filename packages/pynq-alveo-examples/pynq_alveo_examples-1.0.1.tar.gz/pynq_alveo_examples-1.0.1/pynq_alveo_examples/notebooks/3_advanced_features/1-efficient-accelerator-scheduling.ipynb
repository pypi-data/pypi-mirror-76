{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Embedded Scheduler\n",
    "\n",
    "Most Alveo shells include a processor on the card to perform low-latency scheduling of accelerators - the _embedded runtime_ or ERT. When version 2.3 or greater of XRT is being used `start` can take an optional `waitfor` keyword parameter that is a list of handles that must complete before the accelerator starts.\n",
    "\n",
    "To show this we are going to use the `mmult` and `vadd` kernels in the advanced designs bitstream to perform a 2x2 tiled matrix multiplication. The core matrix kernel performs a fixed 512 x 512 multiplication so the final result will be a 1024 x 1024 matrix multiplication. As this operation requires 8 invocations of the `mmult` kernel and 4 invocations of the `vadd` with some data-dependencies between them it provides a good example for performing this type of work.\n",
    "\n",
    "The rest of this notebook is split into 3 sections: first we'll map out the algorithm and implement in software; next we'll use the standard `call` and `start` functions without the `waitfor` to perform the multiplication in hardware; and finally we'll use the embedded scheduler and see how much the performance increases.\n",
    "\n",
    "## Setting up the data structures\n",
    "\n",
    "First we need to create our test matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "KERNEL_SIZE = 512\n",
    "KERNEL_SHAPE = (KERNEL_SIZE, KERNEL_SIZE)\n",
    "MAT_SHAPE = (KERNEL_SIZE * 2, KERNEL_SIZE * 2)\n",
    "\n",
    "in_a = np.random.randint(100, size=MAT_SHAPE, dtype='i4')\n",
    "in_b = np.random.randint(100, size=MAT_SHAPE, dtype='i4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define our tiled matrices. Structurally we can think of our original matrix as having dimensions $(2 * \\textit{tile_size}, 2 * \\textit{tile_size})$ and what we would like is a $(2, 2)$ matrix where each element is $(\\textit{tile_size}, \\textit{tile_size})$.\n",
    "\n",
    "![tiling](img/tiling.png)\n",
    "\n",
    "Looking at this from the point of view of the memory layout of the matrix we can see that the tiles in the original matrix are laid out in an interleaved fashion.\n",
    "\n",
    "![interleaved matrix](img/layout.png)\n",
    "\n",
    "Our accelerator needs each tile to be contiguous so we can perform the appropriate shuffle using the `transpose` function offered by numpy.\n",
    "\n",
    "![shuffled matrix](img/layout_shuffle.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tiles_a = np.ndarray(shape=(2, 2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "in_tiles_b = np.ndarray(shape=(2, 2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "\n",
    "in_tiles_a[:] = np.transpose(in_a.reshape(2, KERNEL_SIZE, 2, KERNEL_SIZE), (0,2,1,3))\n",
    "in_tiles_b[:] = np.transpose(in_b.reshape(2, KERNEL_SIZE, 2, KERNEL_SIZE), (0,2,1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to need some space for temporarily storing the output of each tiled multiplication and the buffer for the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_buf = np.ndarray(shape=(2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "out_tiles = np.ndarray(shape=(2,2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tiled algorithm in software\n",
    "\n",
    "Before we start running in hardware we want to make sure that our tiling algorithm runs in software correctly. Our algorithm is a a simple 2x2 matrix multiplication on blocks where each output tile requires 2 matrix multiplications and an addition\n",
    "\n",
    "![matrix multiplication](img/tiled_mult.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        temp_buf[0] = in_tiles_a[i,0] @ in_tiles_b[0,j]\n",
    "        temp_buf[1] = in_tiles_a[i,1] @ in_tiles_b[1,j]\n",
    "        out_tiles[i,j] = temp_buf[0] + temp_buf[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the result we can undo the transpose and check to make sure that result matches a plain matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out_tiles.transpose(0,2,1,3).reshape(2*KERNEL_SIZE,2*KERNEL_SIZE)\n",
    "np.array_equal(in_a @ in_b, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to hardware\n",
    "\n",
    "Now we've validated the concept we can try running our hardware. The bitstream we are going to use has two cores - a matrix multiplication core and vector addition core - both hard coded for 512x512 matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pynq\n",
    "ol = pynq.Overlay('advanced.xclbin')\n",
    "\n",
    "mmult = ol.mmult_1\n",
    "vadd = ol.vadd_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create new input data and create our buffers using `pynq.allocate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_a = np.random.randint(100, size=MAT_SHAPE, dtype='i4')\n",
    "in_b = np.random.randint(100, size=MAT_SHAPE, dtype='i4')\n",
    "\n",
    "in_tiles_a = pynq.allocate(shape=(2, 2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "in_tiles_b = pynq.allocate(shape=(2, 2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "temp_buf = pynq.allocate(shape=(2, 2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "out_tiles = pynq.allocate(shape=(2,2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "\n",
    "in_tiles_a[:] = np.transpose(in_a.reshape(2, KERNEL_SIZE, 2, KERNEL_SIZE), (0,2,1,3))\n",
    "in_tiles_b[:] = np.transpose(in_b.reshape(2, KERNEL_SIZE, 2, KERNEL_SIZE), (0,2,1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sync the input buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tiles_a.sync_to_device()\n",
    "in_tiles_b.sync_to_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loop looks identical to the software version execpt we are calling the accelerators rather than using the numpy operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        mmult.call(in_tiles_a[i,0], in_tiles_b[0,j], temp_buf[0])\n",
    "        mmult.call(in_tiles_a[i,1], in_tiles_b[1,j], temp_buf[1])\n",
    "        vadd.call(temp_buf[0], temp_buf[1], out_tiles[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to retrieve the output tiles from the PCIe card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tiles.sync_from_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the result with software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(out_tiles.transpose(0,2,1,3).reshape((1024,1024)), in_a @ in_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlapping Execution\n",
    "\n",
    "In the previous implementation each accelerator execution is synchronous. Visualsing this as a schduling diagram we can see the gaps in the matrix multiplication kernel's utilisation we should be able to exploit. Each iteration of the inner loop is represented by a different colour. The vadd kernel does not require as much time as the mmult so is shown as taking half the time for simplicity.\n",
    "\n",
    "![plain schedule](img/schedule_plain.png)\n",
    "\n",
    "Ideally what we would prefer is that the next iteration can start work on performing the matrix multiplication while the previous iteration performs the addition. That way the multiplication kernel is never idle.\n",
    "\n",
    "![overlapped schedule](img/schedule_overlap.png)\n",
    "\n",
    "We could perform this manually but it's a lot easier and less error prone to use the `waitfor` functionality present in PYNQ.\n",
    "\n",
    "First we need to create a bigger temporary buffer so that we can run two iterations simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_buf = pynq.allocate(shape=(2, 2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can execute our previous loop using `start` functions using the `waitfor` parameter to enforce the data-dependence. We add an additional dependency between the vadd iterations to ensure that when the final loop iteration is waited on we know the entire computation is finished. Note that `None` is a valid handle to pass making our inter-loop dependency simpler to express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_wh = None\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        wh1 = mmult.start(in_tiles_a[i,0], in_tiles_b[0,j], temp_buf[j,0])\n",
    "        wh2 = mmult.start(in_tiles_a[i,1], in_tiles_b[1,j], temp_buf[j,1])\n",
    "        sum_wh = vadd.start(temp_buf[j,0], temp_buf[j,1], out_tiles[i,j],\n",
    "                            waitfor=(wh1, wh2, sum_wh))\n",
    "sum_wh.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the relative performance improvement using the `%%timeit` magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.9 ms ± 3.56 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        mmult.call(in_tiles_a[i,0], in_tiles_b[0,j], temp_buf[0])\n",
    "        mmult.call(in_tiles_a[i,1], in_tiles_b[1,j], temp_buf[1])\n",
    "        vadd.call(temp_buf[0], temp_buf[1], out_tiles[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.7 ms ± 21.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "sum_wh = None\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        wh1 = mmult.start(in_tiles_a[i,0], in_tiles_b[0,j], temp_buf[j,0])\n",
    "        wh2 = mmult.start(in_tiles_a[i,1], in_tiles_b[1,j], temp_buf[j,1])\n",
    "        sum_wh = vadd.start(temp_buf[j,0], temp_buf[j,1], out_tiles[i,j],\n",
    "                            waitfor=(wh1, wh2, sum_wh))\n",
    "sum_wh.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple modification to our code to use the embedded scheduler has resulted in a 15% reduction of execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "Finally we need to free the resources to let other processes use the FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del in_tiles_a\n",
    "del in_tiles_b\n",
    "del temp_buf\n",
    "del out_tiles\n",
    "ol.free()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2019 Xilinx, Inc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# -*- coding: utf-8 -*-
import numpy as np

import tensorflow as tf
from tensorflow import keras


def energy_fun(Jz, Bx, samples, queue_samples, log_probs_fun, log_probs_array):
    """Calculate local energies of `samples` for open boundary conditions.

    Parameters
    ----------
    Jz : (N-1,) array_like
        An array containing the couplings between neighbouring spins in
        z-direction. `N` is the system size.
    Bx : float
        The transverse magnetic field strength.
    samples : (num_samples, N) tensorflow.Tensor
        An array containing the samples for which the energy should be
        calculated. `num_samples` is the number of samples and `N` is the
        system size.
    queue_samples : ((N+1), num_samples, N) ndarray
        An empty pre-allocated array for storing non-diagonal elements.
    log_probs_fun : callable
        A function that returns the log probabilities of a given spin
        configuration. This function is implemented by the wavefunction model,
        because it needs to access the underlying RNN.
    log_probs_array : ((N+1)*num_samples) ndarray
        An empty pre-allocated array for storing the log-probabilities of
        non-diagonal elements. `num_samples` is the number of samples and
        `N` is the system size.

    Returns
    -------
    local_energies: (num_samples,) ndarray
        The local energy for each of the provided samples.

    """
    num_samples = samples.shape[0]
    N = samples.shape[1]

    local_energies = np.zeros((num_samples), dtype=np.float64)

    for i in range(N - 1):  #diagonal elements
        values = samples[:, i] + samples[:, i + 1]
        valuesT = np.empty_like(values, dtype=np.float64)
        valuesT[(values == 2).numpy()] = +1  #If both spins are up
        valuesT[(values == 0).numpy()] = +1  #If both spins are down
        valuesT[(values == 1).numpy()] = -1  #If they are opposite

        local_energies += valuesT * (-Jz[i])

    queue_samples[0] = samples  #storing the diagonal samples

    if Bx != 0:
        for i in range(N):  #Non-diagonal elements
            valuesT = np.copy(samples)
            valuesT[:, i][(samples[:, i] == 1).numpy()] = 0  #Flip
            valuesT[:, i][(samples[:, i] == 0).numpy()] = 1  #Flip

            queue_samples[i + 1] = valuesT

    #Calculating log_probs from samples
    #Do it in steps

    # print("Estimating log probs started")
    # start = time.time()

    len_sigmas = (N + 1) * num_samples
    steps = len_sigmas // 25000 + 1  #I want a maximum of 25000 in batch size just to not allocate too much memory

    queue_samples_reshaped = np.reshape(queue_samples,
                                        [(N + 1) * num_samples, N])
    for i in range(steps):
        if i < steps - 1:
            cut = slice((i * len_sigmas) // steps,
                        ((i + 1) * len_sigmas) // steps)
        else:
            cut = slice((i * len_sigmas) // steps, len_sigmas)
        log_probs_array[cut] = log_probs_fun(queue_samples_reshaped[cut])

    # end = time.time()
    # print("Estimating log probs ended ", end-start)

    log_probs_reshaped = np.reshape(log_probs_array, [N + 1, num_samples])
    for j in range(num_samples):
        local_energies[j] += -Bx * np.sum(
            np.exp(0.5 * log_probs_reshaped[1:, j] -
                   0.5 * log_probs_reshaped[0, j]))

    return local_energies


class StackedGRU(tf.keras.layers.Layer):
    """A stacked RNN with GRU layers, implemented as a single layer.

    This keras layer implements a stacked Recurrent Neural Network, using Gated
    Recurrent Units as its building blocks. As of tensorflow 2.2, using the
    `StackedRNNCells`_ layer provided by keras does not allow using CuDNN, as
    it is `only available at the layer level, and not at the cell level`_.
    Therefore, we have to manually build a layer consisting of multiple GRU
    layers.

    The basics of the implementation are heavily inspired by this `question on
    stackoverflow`_.

    Notes
    -----
    Currently, the states of the Gated Recurrent Units are handled manually in
    the Attribute `gru_states`. Using the ``stateful`` property provided by
    `tensorflow.keras.layers.GRU` is inconvenient, because, according to
    `the documentation`_, one would need to fix the number of samples that
    can be generated by the RNN (i.e. the ``batch_size`` in Machine
    Learning terminology) at the time of initializing the Gated Recurrent
    Unit.

    .. _StackedRNNCells:
        https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/StackedRNNCells
    .. _only available at the layer level, and not at the cell level:
        https://www.tensorflow.org/guide/keras/rnn
    .. _question on stackoverflow:
        https://stackoverflow.com/questions/61048926/tensorflow-2-gru-layer-with-multiple-hidden-layers
    .. _the documentation:
        https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/RNN

    """
    def __init__(self, hidden_dims, input_dim, **kwargs):
        """Initialize an instance of StackedGRU.

        Parameters
        ----------
        hidden_dims : (num_layers,) array_like
            A list containing the number of hidden units for each of the GRU
            layers. `num_layers` is the number of layers in the stacked RNN.
        input_dim : int
            The dimension of the underlying Hilbert space, which is also the
            length of the vectors holding the one-hot encoded spins.
        **kwargs
            Keyword arguments to be passed to the parent class
            `tensorflow.keras.layers.Layer`.

        """
        super(StackedGRU, self).__init__(**kwargs)
        self.hidden_dims = hidden_dims
        self.input_dim = input_dim

        self.num_layers = len(hidden_dims)
        """int: The number of layers in the stacked RNN."""
        def layer(hidden_dim, input_dim):
            """CuDNN-compatible GRU-layer, with time-major inputs.

            Parameters
            ----------
            hidden_dim : int
                Number of hidden units for the layer.
            input_dim : int
                The number of „features“ in Machine Learning terminology.

            Returns
            -------
            gru_layer : tensorflow.keras.layers.GRU
                A layer holding a Gated Recurrent Unit, with settings that are
                compatible with the CuDNN-kernel provided by keras. The inputs
                are specified as being in „time-major“ ordering, which means
                the expected input-dimensions are ``[1, num_samples,
                input_dim]`` corresponding to ``[time_step, batch_size,
                feature]``.

                For our use case, the RNN processes only one „time step“ at a
                time, because the next sample needs to be generated from the
                previous RNN output and is not known at the beginning.

            """
            return tf.keras.layers.GRU(hidden_dim,
                                       activation='tanh',
                                       recurrent_activation='sigmoid',
                                       use_bias=True,
                                       recurrent_dropout=0.0,
                                       return_state=True,
                                       stateful=False,
                                       unroll=False,
                                       time_major=True,
                                       reset_after=True,
                                       input_shape=(None, input_dim))

        self.list_of_gru = [layer(hidden_dims[0], input_dim)]
        """list of tensorflow.keras.layers.GRU: A list that keeps track of the
        Gated Recurrent Units in the stacked RNN. 
        """
        if len(hidden_dims > 1):
            # If there is more than one Gated Recurrent Unit, the first layer
            # has the dimension of the underlying Hilbert space as input_dim,
            # while every further layer has the hidden_dim of the previous
            # layer as its input_dim.
            self.list_of_gru += [
                layer(dim, hidden_dims[i])
                for i, dim in enumerate(hidden_dims[1:])
            ]

        self.gru_states = None
        """list of tensorflow.Tensor: This keeps track of the current state of
        each Gated Recurrent Unit.
        """

    def reset_states(self, states=None):
        """Reset the states of the Gated Recurrent Units.

        Parameters
        ----------
        states : list of tensorflow.Tensor, optional
            If you specify explicit states, these will be used as the new
            states of the RNN. Otherwise, the default zero states will be used.

        """
        self.gru_states = states

    def zero_gru_states(self, inputs):
        """Get initial states filled with zeros for each Gated Recurrent Unit.

        Parameters
        ----------
        inputs : (1, num_samples, input_dim) array_like
            An array that with a shape matching the shape of the input to the
            RNN. `num_samples` is the number of samples and `input_dim` is
            the dimension of the underlying Hilbert space.

        Returns
        -------
        states : list of tensorflow.Tensor
            A list of zero-filled initial states for every Gated Recurrent Unit
            in the stacked RNN.

        """
        states = [i for i in range(self.num_layers)]
        states[0] = self.list_of_gru[0].get_initial_state(inputs)
        if self.num_layers > 1:
            for i in range(self.num_layers - 1):
                gru = self.list_of_gru[i + 1]
                # time_major=True -> (1, num_samples, spin_dim/hidden_dim)
                dummy_input = tf.zeros(shape=(1, inputs.shape[1],
                                              self.hidden_dims[i]))
                states[i + 1] = gru.get_initial_state(dummy_input)
        return states

    def call(self, inputs, initial_state=None, training=None):
        """Pass inputs through the stacked RNN.

        Parameters
        ----------
        inputs : (1, num_samples, input_dim) tensorflow.Tensor
            Input to the stacked RNN, encoding the current spin-configuration.
            `num_samples` is the number of samples being processed and
            `input_dim` is the dimension of the underlying Hilbert space.
        initial_state : list of tensorflow.Tensor, optional
            If provided, the RNN will be called with inital_state as its
            initial state. Useful for continuing execution from a specified
            configuration.
        traning : bool, optional
            This parameter is unused. It is only in the function signature to
            support keras' `built-in training and evaluation loops`_.

        .. _built-in training and evaluation loops:
            https://www.tensorflow.org/guide/keras/custom_layers_and_models

        """
        seqs = inputs
        if initial_state is not None:
            self.gru_states = initial_state
        if initial_state is None and self.gru_states is None:
            # If there is no recorded state and no states are provided,
            # initialize the stacked RNN with zero-states.
            self.gru_states = self.zero_gru_states(inputs)

        for i in range(self.num_layers):
            gru = self.list_of_gru[i]
            state = self.gru_states[i]
            (seqs, state) = gru(inputs=seqs, initial_state=state)
            if i < self.num_layers - 1:
                seqs = tf.expand_dims(seqs, 0)
            self.gru_states[i] = state
        return seqs


class NeuralWavefunction1D(tf.keras.Model):
    """A keras model for neural wavefunctions, using a stacked RNN.

    This model consists of a stacked RNN with Gated Recurrent Units as
    building blocks, followed by a fully connected layer that outputs
    an un-normalized logit-distribution over the possible spin-orientations for
    the next spin sample.

    """

    def __init__(self, systemsize, hidden_dims, input_dim=2, Jz=[1], Bx=1, name='nwf_1d', **kwargs): # yapf: disable
        """Initialize an instance of a neural wavefunction.

        Parameters
        ----------
        systemsize : int
            The length of the spin-chain.
        hidden_dims : (num_layers,) array_like
            A list containing the number of hidden units for each of the GRU
            layers making up the stacked RNN. `num_layers` is the number of
            layers in the RNN.
        input_dim : int, optional
            The dimension of the underlying Hilbert space. Defaults to 2.
        Jz : (N-1,) list of int or (1,) list of int, optional
            An array containing the couplings between neighbouring spins in
            z-direction. It can either be provided explicitly for each pair in
            the spin-chain or as an array of length one. In the second case,
            the same coupling is used for the whole chain. Defaults to 1 for
            the whole chain. `N` is the system size.
        Bx : float, optional
            The transverse magnetic field strength. Defaults to 1.
        name : str, optional
            A name for the model that is passed to tensorflow. Used when
            displaying diagrams and operations.
        **kwargs
            Keyword arguments to be passed to the parent class
            `tensorflow.keras.Model`.

        """
        super(NeuralWavefunction1D, self).__init__(name=name, **kwargs)
        if len(Jz) == 1:
            Jz = Jz * systemsize
        self.N = systemsize
        self.input_dim = input_dim
        self.Jz = Jz
        self.Bx = Bx

        self.rnn = StackedGRU(hidden_dims, name='RNN', dtype=tf.float64)
        """StackedGRU: The stacked RNN making up the first part of the neural
        wavefunction."""

        self.dense = keras.layers.Dense(input_dim, name='wf_dense', dtype=tf.float64) # yapf: disable
        """tensorflow.keras.layers.Dense: The fully connected layer that
        transforms RNN output to an un-normalized logit-distribution over
        possible spin-orientations."""

    #@tf.function
    def log_probability(self, samples):
        """Calculate the log-probabilities of obtaining `samples`.

        Parameters
        ----------
        samples : (num_samples, N) tensorflow.Tensor
            An array containing the samples for which log-probabilities should
            be caculated. `num_samples` is the number of samples and `N` is
            the system size.

        Returns
        -------
        log_probabilities : (num_samples,) tensorflow.Tensor
            The log-probability of obtaining each sample.

        """
        num_samples = samples.shape[0]
        inputs = tf.zeros(shape=(1, num_samples, self.input_dim),
                          dtype=tf.float64)

        log_probs = []

        self.rnn.reset_states()

        for n in range(self.N):
            rnn_output = self.rnn(inputs)
            output = self.dense(rnn_output)
            # Use the logsumexp trick to convert the unnormalized log-odds from
            # the rnn to logarithms of normalized probabilities.
            log_prob = output - tf.repeat(
                tf.reduce_logsumexp(output, -1, keepdims=True), 2, axis=1)
            log_probs.append(log_prob)
            inputs = tf.expand_dims(
                tf.one_hot(tf.reshape(tf.slice(
                    samples,
                    begin=[np.int32(0), np.int32(n)],
                    size=[np.int32(-1), np.int32(1)]),
                                      shape=[num_samples]),
                           depth=self.input_dim), 0)

        # There is no need for the cast if the backend is set to float64
        #log_probs=tf.cast(tf.transpose(tf.stack(values=log_probs,axis=2),perm=[0,2,1]),tf.float64)
        log_probs = tf.transpose(tf.stack(values=log_probs, axis=2),
                                 perm=[0, 2, 1])
        one_hot_samples = tf.one_hot(samples,
                                     depth=self.input_dim,
                                     dtype=tf.float64)

        log_probs = tf.reduce_sum(tf.multiply(log_probs, one_hot_samples),
                                  axis=[1, 2])

        return log_probs

    def call(self, num_samples):
        """Call the model to generate spin samples.

        Parameters
        ----------
        num_samples : int
            How many samples should be generated.

        Returns
        -------
        samples : (num_samples, N) tensorflow.Tensor
            An array containing the generated spin samples. `num_samples` is
            the number of samples and `N` is the system size.

        """
        inputs = tf.zeros(shape=(1, num_samples, self.input_dim),
                          dtype=tf.float64)
        x = inputs
        samples = []
        for i in range(self.N):
            x = self.rnn(x)
            x = self.dense(x)
            sample = tf.reshape(tf.random.categorical(x, num_samples=1), [-1])
            samples.append(sample)
            x = tf.one_hot(sample, depth=self.input_dim)
            x = tf.expand_dims(x, 0)
        samples = tf.stack(values=samples, axis=1)
        self.rnn.reset_states()

        return samples
